(defpackage robust-q-learning 
  (:use common-lisp
	expl-policy
	fn-approx
	sampling
	utils)
  (:export robust-q))


(in-package robust-q-learning)


(defconstant *env-reset-max* 100)

(defun robust-q (env num-steps hist-length discount eta-fn &key (samples nil))
  "robust-q ENV NUM-STEPS HIST-LENGTH DISCOUNT ETA-FN &key ENV SAMPLES.  Run our candidate robust learning algorithm and return vector of length HIST-LENGTH representing snapshots of Q-function at regular intervals.  ETA-FN should be a function from TEMPORALDIFF,TIMESTEP,STATE,ACT,NEXT-STATE to ETA.  If SAMPLES is not provided, samples are generated by running in the environment ENV.  If SAMPLES is provided, it should be a vector of length >= NUM-STEPS of elements of the form (S A R NEXT-S)."
  (if samples
      (robust-q-inner num-steps hist-length discount eta-fn
		      (let ((i 0))
			(lambda ()
			  (let* ((sample (aref samples i))
				 (s (first sample))
				 (a (second sample))
				 (r (third sample))
				 (next-s (fourth sample))
				 (actions (env:avail-actions env next-s)))
			    (incf i)
			    (values s a r next-s actions)))))
    (progn 
      (env:reset env)
      (robust-q-inner num-steps hist-length discount eta-fn
		      (lambda ()
			(loop while (env:at-terminal-state env)
			    for i from 0
			    do (env:reset env)
			       (assert (< i *env-reset-max*) nil "unable to reset environment"))
			(let* ((s (env:get-state env))
			       (actions (env:get-actions env))
			       (a (aref actions (random (length actions)))))
			  (multiple-value-bind (r s2)
			      (env:do-action env a)
			    (values s a r s2 (env:get-actions env)))))))))




(defun robust-q-inner (num-steps hist-length discount eta-fn sampler)  
  (loop
      with inc = (floor (/ (1- num-steps) (1- hist-length)))
      with hist = (make-array hist-length :fill-pointer 0)
      with q = (make-instance '<tabular-fn-approx>)
	       
      for step from 1 to num-steps

			 
      do (multiple-value-bind (s a r s2 next-actions)
	     (funcall sampler)
	   (let* ((d (+ r 
			(loop
			    for a2 across next-actions
			    maximizing (* discount (evaluate q (list s2 a2))))
			(- (evaluate q (list s a)))))
		  (eta (funcall eta-fn d num-steps s a s2)))
	     (update q (list s a) (+ d (evaluate q (list s a))) eta)))
	     
      when (= 0 (mod (- num-steps step) inc))
      do (vector-push (get-params q) hist)
	 
      finally (return hist)))


(in-package cl-user)